---
title: "TrainClassifier"
output: html_document
---

```{r setup, include=FALSE}
for(p in c("caret","devtools","xts", "dimRed", "foreach","tidyr","fastICA", "e1071"))
if (!require(p,character.only=TRUE)) {
    install.packages(p)
    require(p,character.only=TRUE)
}

if (!require("influxdbr2",character.only=TRUE)) {
devtools::install_github("mvadu/influxdbr2")
  require("influxdbr2",character.only=TRUE)
}
```


# Database connection

```{r cars}
con <- influx_connection(host = "css20.dmz.teco.edu", scheme="http", port=80) 
result=influx_query_xts(con,db="browser", "select * FROM devicemotion GROUP BY label, subject")
```

# Transform data to table

```{r}
xts_as_table <- function(xts_file){
  # loop over every element of xts_file
  res <- foreach(i=1:length(result),.combine = rbind) %do% {
      # apply split to do windowing: split by seconds
      foreach(s=split(result[i][[1]]$values,"second",k=2),.combine = rbind) %do% {
          # intitialize r
          r={}
          # add subject and label
          r$subject=result[i][[1]]$tags$subject 
          r$label=result[i][[1]]$tags$label
          # extract acceleration coordinates x,y,z
          x=as.numeric(s$accelerationIncludingGravity.x)
          y=as.numeric(s$accelerationIncludingGravity.x)
          z=as.numeric(s$accelerationIncludingGravity.z)
          
          # calculate rmse from x,y,z
          three_d=apply(cbind(x,y,z),FUN=function(x) sqrt(sum(x^2)),MARGIN=1)
          
          # add mobile
          r$mobile=as.vector(first(s$mobile)) 
          # calculate means and add them
          r$mean_x=mean(x,na.rm = TRUE)
          r$mean_y=mean(y,na.rm = TRUE)
          r$mean_z=mean(z,na.rm = TRUE)
          r$mean_3d=mean(three_d,na.rm = TRUE)
          
          # calclate variances and add them
          r$var_x=var(x, na.rm = TRUE)
          r$var_y=var(y,na.rm = TRUE)
          r$var_z=var(z, na.rm = TRUE)
          r$var_3d=var(three_d, na.rm = TRUE)
          
          # calculate ranges and add them
          r$minmax_x=max(x,na.rm = TRUE)-min(x,na.rm = TRUE)
          r$minmax_y=max(y,na.rm = TRUE)-min(y,na.rm = TRUE)
          r$minmax_z=max(z,na.rm = TRUE)-min(z,na.rm = TRUE)
          r$minmax_3d=max(three_d,na.rm = TRUE)-min(three_d,na.rm = TRUE)
          
          # return as df to rbind
          as.data.frame(r)
        }
  }
  return(as.data.frame(res))
}

data <- xts_as_table(result)
```


```{r}
# inspect data frame dim and basic statistics
dim(data)
summary(data)
```


# Data Cleansing

```{r}
# remove testing
data <- dplyr::filter(data, label!="testing")
```

```{r}
# The function IQR.outliers detects outliers
IQR.outliers <- function(data) {
  Q3<-quantile(data,0.75)
  Q1<-quantile(data,0.25)
  IQR<-(Q3-Q1)
  left<- Q1-(1.5*IQR)
  right<- Q3+(1.5*IQR)
  print(sum(data < left))
  print(sum(data > right))
  index <- data < left | data>right
  return(which(index))
}
```



```{r}
# create new frame to visualize remaining NA values
data_NA <- data[rowSums(is.na(data)) > 0,]

# show dropped NA-Values
data_NA

# remove NA values
clean_data <- drop_na(data)

```


```{r}
# outlier removal
clean_data <- clean_data[clean_data$minmax_3d < (mean(clean_data$minmax_3d) + 2* sd(clean_data$minmax_3d)),]
clean_data <- clean_data[clean_data$var_3d < (mean(clean_data$var_3d) + 2* sd(clean_data$var_3d)),]
clean_data <- clean_data[clean_data$mean_3d < (mean(clean_data$mean_3d) + 2* sd(clean_data$mean_3d)),]
```

```{r}
# scaling
prep <- preProcess(clean_data[,-c(1,2,3)], method = c("scale","center"), n.comp=3)
clean_data <- cbind(clean_data[,c(1,2,3)], predict(prep, clean_data[,-c(1,2,3)]))
```


# Visualizations
```{r}
# Boxplot of 3d features
cols=c(7,11,15)
caret::featurePlot(x=clean_data[,cols], y=factor(clean_data$label), plot="box", auto.key = list(columns = 2))

```


# Exercise 4

Create a Leave-one-Subject-out train and test split

```{r}
# Leave-one-subject out train test split
subjects <- levels(factor(clean_data$subject))
data_subject <- vector(mode = "list", length = nlevels(factor(clean_data$subject)))

for (s in seq(1, nlevels(factor(clean_data$subject)))) {
  data_subject[[s]]<- which(clean_data$subject != subjects[s])
}
```

Train a Naive Bayes Classifier
```{r}
# train a Naive Bayes classifier using LOSO-split
train(clean_data[,-c(1,2,3)], clean_data[,"label"], method = "lssvmRadial",
      trControl =  trainControl(index = data_subject, summaryFunction = multiClassSummary))
```

Define trainControl functions
```{r}
# use different train control functions:
# 1. pre-defined function
trainControl1 <- trainControl(index = data_subject, summaryFunction = multiClassSummary,
                              verboseIter = T)
# 2. own function: 
trainControl2 <- trainControl(method = "LGOCV", summaryFunction = multiClassSummary, 
                              classProbs = T)
```

Train different Models using the trainControl functions 
```{r}
# used methods (accuracy): naive_bayes (0.71), ranger (0.76), svmRadial (0.58), svmLinear (0.65), 
# xgbTree (0.77: eta = 0.3, max_depth = 2, colsample_bytree = 0.8, subsample = 0.5, nrounds = 50)
model_naiveBayes <- train(clean_data[,-c(1,2,3)], clean_data[,"label"], method = "naive_bayes",
      trControl =  trainControl1)

model_ranger <- train(clean_data[,-c(1,2,3)], clean_data[,"label"], method = "ranger",
                          trControl =  trainControl1)
```

```{r}
#TODO add more feature manually to improve your results

```


Select best feature using a wrapper using carets Recursive Feature Elimination
```{r}
# define the control using a random forest selection function
control <- rfeControl(functions=nbFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(clean_data[,-c(1,2,3)], factor(clean_data[,"label"]), sizes = c(1:12), rfeControl=control)
# summarize the results
print(results)  
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))  
```

